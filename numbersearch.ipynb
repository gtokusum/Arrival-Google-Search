{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying results from Google query and using logistic regression to predict likelyhood of VIP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will need to create a training dataset. Could integrate ChatGPT using snippets or websites from the following promt: \"Using the following snippets. Give a rating from 0-100 on the likeliness that the person is famous. Respond only with the rating. snippets: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import instaloader\n",
    "import requests\n",
    "\n",
    "#load evn variables\n",
    "API_KEY = os.environ['GOOGLE_API_KEY']\n",
    "SEARCH_ENGINE_ID = os.environ['SEARCH_ENGINE_ID']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Quantify Google query results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions\n",
    "\n",
    "\n",
    "#google name and output json file\n",
    "#takes in name api_key, search_engine_id\n",
    "def search_name(name, api_key, search_engine_id):   \n",
    "    # Iterate over the names and grab search results using Google Custom Search API\n",
    "    search_url = f'https://www.googleapis.com/customsearch/v1?q={name}&cx={search_engine_id}&key={api_key}'\n",
    "    response = requests.get(search_url)\n",
    "    data = json.loads(response.text)\n",
    "    return data\n",
    "\n",
    "\n",
    "# checks email domain for company emails, \n",
    "# input takes in dataframe object\n",
    "# returns dataframe with 1 or 0 for check email column \n",
    "def check_email(df):\n",
    "    xlst = ['graduatehotels.com','schultehospitality.com','schultedc.com']\n",
    "    lst = []\n",
    "    for i in range(len(df)):\n",
    "        if df.loc[i] == \"nan\":\n",
    "            lst.append(0)\n",
    "        else:\n",
    "            if df.loc[i].split(\"@\")[1].lower() in xlst:\n",
    "                lst.append(1)\n",
    "            else:\n",
    "                lst.append(0)\n",
    "    return lst\n",
    "\n",
    "\n",
    "#clean data set to add full names and split email for domain, \n",
    "# takes in string file path\n",
    "# return dataframe object\n",
    "def clean_data(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    df['names'] = df['First Name'] + \" \" + df['Last Name']\n",
    "    df['Email'].fillna(\"nan\",inplace=True)\n",
    "    lenOfDF = [0 for i in range(len(df))]\n",
    "    emails = df['Email']\n",
    "    df['Email Check'] = check_email(emails)\n",
    "    df['Websites'] = lenOfDF\n",
    "    df['Social Media'] = lenOfDF\n",
    "    df['keywords'] = lenOfDF\n",
    "    return df\n",
    "\n",
    "\n",
    "# instagram follower count\n",
    "# takes in url of account from json['items'][int]['metatags']['og:url']\n",
    "#  returns int follower count,\n",
    "# WILL NEED TWEEPY FOR TWITTER FOLLOWER COUNT, DO NOT HAVE ACCESS YET\n",
    "def instaCount(link):\n",
    "    bot = instaloader.Instaloader()\n",
    "    profile = instaloader.Profile.from_username(bot.context,link[26:-1])\n",
    "    return profile.followers\n",
    "\n",
    "\n",
    "# scans json for websites associated with name \n",
    "# takes in jsonObj and scans through for specific links\n",
    "# links will be weighted using ints will need to tune weights later\n",
    "# returns wighted score \n",
    "def scan_websites(jsonObj,firstName,lastName):\n",
    "    WEBSITES = {'en.wikipedia.org':9,'www.forbes.com':10,'www.bloomberg.com':9,'www.imdb.com':9,'sanfran.com':3,'www.architecturaldigest.com':5,\n",
    "                'www.tvinsider.com':5,'nypost.com':8,'www.themoviedb.org':7,'www.celebritynetworth.com':6,'www.tvguide.com':6,\n",
    "                'ew.com':6,'www.thefamouspeople.com':8,'www.newyorker.com':8,'www.britannica.com':8,'www.investopedia.com':5,'abc.com':10,\n",
    "                'www.cnbc.com':10,'www.nbc.com':10,'www.pbs.org':10,'www.espn.com':10,'www.biography.com':8,'www.newsweek.com':6,'www.cbsnews.com':10} # add the rest of links\n",
    "    SOCIAL = ['www.instagram.com']\n",
    "    score = 0\n",
    "    k = 10 #only checks first 10, change for more, can do len for full\n",
    "    first = firstName.lower()\n",
    "    last = lastName.lower()\n",
    "    social = False\n",
    "    socInd = []\n",
    "    if 'items' in jsonObj:\n",
    "        for j in range(0,k): #only checks first 10\n",
    "            if jsonObj['items'][j]['displayLink'] in WEBSITES:\n",
    "                score += WEBSITES[jsonObj['items'][j]['displayLink']]\n",
    "            if first in jsonObj['items'][j]['displayLink'].lower() or last in jsonObj['items'][j]['displayLink'].lower():\n",
    "                score += 10 # adjust for weight\n",
    "            if jsonObj['items'][j]['displayLink'] in SOCIAL:\n",
    "                social = True\n",
    "                socInd.append(j)\n",
    "    return score ,social,socInd\n",
    "\n",
    "\n",
    "#scans for keywords in snippet \n",
    "#will take in jsonObj and look for key words in the snippet\n",
    "#could also weigh words \n",
    "#can also combine with scan websites \n",
    "#outputs int score\n",
    "def scan_keywords(jsonObj):\n",
    "    KEYWORDSW = {'owner':10,'died':-100_000_000,'dead':-100_000_000,'passed':-100_000_000} #add more words. CAN ADD NEGATIVE FOR DIED/DEATH/PASSING\n",
    "    KEYWORDS = ['onwer','actress','owner','entrepenuer','CEO','NBA','NFL','MLB','boradcaster','founder','president'] #add more words\n",
    "    k = 10 #only checks first 10, change for more, can do len for full\n",
    "    score = 0\n",
    "    if 'items' in jsonObj:\n",
    "        for j in range(0,k): #only checks first 10\n",
    "            for x in jsonObj['items'][j]['snippet'].split(\" \"):\n",
    "                if x.lower() in KEYWORDS:\n",
    "                    score +=1 #comment code for dictionary\n",
    "                    # score += KEYWORDSW[x.lower()] for dictoinary\n",
    "    return score\n",
    "\n",
    "#scans json for websites and keywords. combines the 2 functions above\n",
    "def scan_combined(jsonObj):\n",
    "    WEBSITES = {'en.wikipedia.org':10} # add the rest of links. CAN INCLUDE NEGATIVE NUMBER WEBSITES (IE. ONION, SNOPS, SATIRE WEBSITES)\n",
    "    KEYWORDS = ['owner'] #{owner:10} #uncomment for dictionary\n",
    "    score = 0\n",
    "    k = 10 #only checks first 10, change for more, can do len for full\n",
    "    name = jsonObj[0]['queries']['request'][0]['searchTerms'].split(\" \")\n",
    "    if 'items' in jsonObj:\n",
    "        for j in range(0,k): #only checks first 10\n",
    "            if jsonObj['items'][j]['displayLink'] in WEBSITES:\n",
    "                score += WEBSITES[jsonObj['items'][j]['displayLink']]\n",
    "            if name[0] in jsonObj['items'][j]['displayLink'] or name[1] in jsonObj['items'][j]['displayLink']:\n",
    "                score += 15 # adjust for weight\n",
    "            for x in jsonObj['items'][j]['snippet'].split(\" \"):\n",
    "                if x.lower() in KEYWORDS:\n",
    "                    score += 10 #comment code for dictionary\n",
    "                    #score += KEYWORDS[x.lower()] #uncomment for dic\n",
    "    return score\n",
    "\n",
    "\n",
    "def search_analyize(df):\n",
    "    for i in range(len(df)):\n",
    "        social = False\n",
    "        data = search_name(df['names'].loc[i],API_KEY,SEARCH_ENGINE_ID)\n",
    "        df.at[i,'Website'], social,inde = scan_websites(data,df['First Name'].loc[i],df['Last Name'].loc[i])\n",
    "        df.at[i,'keywords'] = scan_keywords(data)\n",
    "        if social:\n",
    "            df.at[i,'Social Media'] = instaCount(data['items'][inde]['metatags']['og:url'])\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = clean_data('test file.xlsx')\n",
    "\n",
    "jsondata = []\n",
    "for i in range(len(test)):\n",
    "    jsondata.append(str(search_name(test['names'].loc[i],API_KEY,SEARCH_ENGINE_ID)))\n",
    "jsondata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = search_name('jack',API_KEY,SEARCH_ENGINE_ID)\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "tt = test['data'].loc[0]\n",
    "print(type(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(jsondata))\n",
    "type(jsondata[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1970777\n"
     ]
    }
   ],
   "source": [
    "print(instaCount('https://www.instagram.com/mcuban/'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Likliness of VIP\n",
    "\n",
    "## Default \n",
    "Will assume people with certain email domains to be considered VIP. \n",
    "Any Sales/Marketing VIPs will also be considered VIP by Default. \n",
    "With HMS we can filter them out by tagging guest profile as VIP\n",
    "\n",
    "## Methodology\n",
    "Using Logistic Regression trained on excel file containing names we know are VIPS and not VIPS. Will run the search function and analyize search data. Using the quantified results from analysis we will train our Logistic Regression model. This way we can hope to reduce false positives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.read('traindata.xlsx')\n",
    "df = clean_data(trainData)\n",
    "df = search_analyize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,X = df['famous'],df.drop('famous',axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = model.score(X_test,y_test)\n",
    "\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
